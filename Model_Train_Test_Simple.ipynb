{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:26.460024Z",
     "start_time": "2024-07-26T16:51:26.433514Z"
    }
   },
   "outputs": [],
   "source": [
    "# note: text\n",
    "# fixed: text\n",
    "# fix me: text\n",
    "# error: text\n",
    "# summary: text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "#Loader function for Pytorch Dataset Folder thingy\n",
    "def load_data(path: str):\n",
    "    #print(f\"Loading file: {path}\")\n",
    "    tensor = torch.load(path)\n",
    "    #tensor = tensor.float()\n",
    "    #print(f\"Loaded tensor type: {type(tensor)} {tensor.type()}\")\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:27.166200Z",
     "start_time": "2024-07-26T16:51:27.061341Z"
    }
   },
   "id": "55baaa3bcda72ffd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ALL CODE ABOVE IS FUNCTIONS FROM MFCC_DATA_IMPORT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb21a47bc95f30a"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import os\n",
    "InputFolder = os.getcwd() + \"/MFCC/tensors3ch/\"\n",
    "#Note: Step 1: Create a dataset from the new organized folder\n",
    "from torchvision import datasets\n",
    "\n",
    "my_data_set = datasets.DatasetFolder(\n",
    "        root = InputFolder,\n",
    "        loader = load_data,\n",
    "        transform = None, #use \"test_transform\" only if using coeff because its missing channels=1 dimention  #our transformation for data\n",
    "        target_transform = None,\n",
    "        extensions = ('.pt',)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:27.168915Z",
     "start_time": "2024-07-26T16:51:27.117641Z"
    }
   },
   "id": "d91b84f887b551f8"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "#Note: Step 2: Determine the sizes for training and testing splits And Split the dataset\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.7 * len(my_data_set))\n",
    "test_size = len(my_data_set) - train_size\n",
    "train_dataset, test_dataset = random_split(my_data_set, [train_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:27.777102Z",
     "start_time": "2024-07-26T16:51:27.761717Z"
    }
   },
   "id": "e97881811308182d"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "#Note:  step 3 - Turn train\\test Datasets into train\\test DataLoaders\n",
    "BATCH_SIZE = 10\n",
    "NUM_OF_WORKERS = os.cpu_count() #this is used on collab when I want to use all max cpu power\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size = BATCH_SIZE,  # how many samples per batch?\n",
    "                              #num_workers=NUM_OF_WORKERS,#ERROR THIS DOESNT WORK ON LAPTOP BUT WORKS ON COLLAB # how many CPU subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle = True)  # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                             batch_size = BATCH_SIZE,  # how many samples per batch?\n",
    "                             #num_workers=NUM_OF_WORKERS,#ERROR THIS DOESNT WORK ON LAPTOP BUT WORKS ON COLLAB # how many CPU subprocesses to use for data loading? (higher = more)\n",
    "                             shuffle = True)  # shuffle the data?"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:28.494098Z",
     "start_time": "2024-07-26T16:51:28.355562Z"
    }
   },
   "id": "9f7f586a6ec2bbb6"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Class Label': Label Number \n",
      "{'airplane': 0, 'breathing': 1, 'brushing_teeth': 2, 'can_opening': 3, 'car_horn': 4, 'cat': 5, 'chainsaw': 6, 'chirping_birds': 7, 'church_bells': 8, 'clapping': 9, 'clock_alarm': 10, 'clock_tick': 11, 'coughing': 12, 'cow': 13, 'crackling_fire': 14, 'crickets': 15, 'crow': 16, 'crying_baby': 17, 'dog': 18, 'door_wood_creaks': 19, 'door_wood_knock': 20, 'drinking_sipping': 21, 'engine': 22, 'fireworks': 23, 'footsteps': 24, 'frog': 25, 'glass_breaking': 26, 'hand_saw': 27, 'helicopter': 28, 'hen': 29, 'insects': 30, 'keyboard_typing': 31, 'laughing': 32, 'mouse_click': 33, 'pig': 34, 'pouring_water': 35, 'rain': 36, 'rooster': 37, 'sea_waves': 38, 'sheep': 39, 'siren': 40, 'sneezing': 41, 'snoring': 42, 'thunderstorm': 43, 'toilet_flush': 44, 'train': 45, 'vacuum_cleaner': 46, 'washing_machine': 47, 'water_drops': 48, 'wind': 49}\n"
     ]
    }
   ],
   "source": [
    "class_names = my_data_set.classes\n",
    "class_dict = my_data_set.class_to_idx\n",
    "print(f\"'Class Label': Label Number \\n{class_dict}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:28.494387Z",
     "start_time": "2024-07-26T16:51:28.358283Z"
    }
   },
   "id": "ae10b1acb2c9da55"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader Samples:\t280*10,\t\tTest DataLoader Samples:\t120*10.\n",
      "Train DataSet Samples:\t\t2800,\t\tTest DataSet Samples:\t\t1200.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train DataLoader Samples:\\t{len(train_dataloader)}*{train_dataloader.batch_size},\\t\\tTest DataLoader Samples:\\t{len(test_dataloader)}*{test_dataloader.batch_size}.\")\n",
    "print(f\"Train DataSet Samples:\\t\\t{len(train_dataset)},\\t\\tTest DataSet Samples:\\t\\t{len(test_dataset)}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:29.312916Z",
     "start_time": "2024-07-26T16:51:29.308772Z"
    }
   },
   "id": "d9d11625dbe18dbb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CREATING A MODEL USING PYTORCH SEQUENIAL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cfce6c3c2d50c01"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Note: Setup device agnostic code - automatically set what cpu\\gpu to use for best performance\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:31.521995Z",
     "start_time": "2024-07-26T16:51:31.472861Z"
    }
   },
   "id": "2ad523a37aaaac70"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:32.284054Z",
     "start_time": "2024-07-26T16:51:32.148466Z"
    }
   },
   "id": "e073483ccd58375e"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "#Summary: Change this model architecture to whatever you want and run all the code below\n",
    "class ModelV1(nn.Module):\n",
    "    def __init__(self,inputSize,outputSize):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nnBlock1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=inputSize,out_channels=40,kernel_size=(8,1),padding=0),\n",
    "                nn.BatchNorm2d(40),\n",
    "                \n",
    "                nn.Conv2d(in_channels=40,out_channels=40,kernel_size=(8,1),padding=0),\n",
    "                nn.BatchNorm2d(40),\n",
    "                \n",
    "                nn.MaxPool2d(1,160),\n",
    "                \n",
    "\n",
    "                \n",
    "                nn.ReLU(),\n",
    "                #nn.MaxPool2d(kernel_size=(4,3),stride=(1,3)),\n",
    "        )    \n",
    "        \n",
    "        self.nnBlock2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=80,out_channels=80,kernel_size=(1,3),padding=0,stride = 1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(1,3),stride=(1,3))\n",
    "        )    \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=160,out_features=100),\n",
    "            #nn.Linear(in_features=5000,out_features=5000),\n",
    "            nn.Linear(in_features=100,out_features=outputSize)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.nnBlock1(x)\n",
    "        #x = self.nnBlock2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ModelV1(inputSize=3,outputSize=50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:33.009789Z",
     "start_time": "2024-07-26T16:51:32.887197Z"
    }
   },
   "id": "4edfb44bcd12e35a"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 3, 498, 13])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: Used for debugging - IF THIS DOESNT RUN - SOMETHING IS WRONG\n",
    "model.state_dict()\n",
    "next(model.modules())\n",
    "model.eval()\n",
    "coeff,y = next(iter(test_dataloader))\n",
    "model_pred_logits = model(coeff).squeeze(dim=0) # make sure image is right shape + on right device\n",
    "model_pred_probs = torch.softmax(model_pred_logits, dim=1)\n",
    "model_pred_label = torch.argmax(model_pred_probs, dim=1)\n",
    "coeff.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:33.755917Z",
     "start_time": "2024-07-26T16:51:33.625122Z"
    }
   },
   "id": "6cbfda5aa4826fa8"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b578d49407909372"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INIT TENSORBOARD & SETUP\n",
    "Dashboard: [http://localhost:6006](http://localhost:6006)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b790c99c63949a3e"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-2c3e05b2e4979ef3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-2c3e05b2e4979ef3\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.utils.tensorboard\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# Stand Tensorboard\n",
    "%tensorboard --logdir=runs --reload_multifile True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:36.724061Z",
     "start_time": "2024-07-26T16:51:36.684582Z"
    }
   },
   "id": "a285cffe5e04b8e6"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# NOTE: SETUP METRICS USING TORCH LIBRARY - NOT SURE IF THIS MAKES IT EASIER OR NOT \n",
    "acc_func = Accuracy(task=\"multiclass\", num_classes=50).to(device)\n",
    "\n",
    "# NOTE: SETUP TQDM TO SHOW A PROGRESS BAR\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "#Summary 1: Create Model\n",
    "model = ModelV1(inputSize=3,outputSize=50)\n",
    "model = model.to(device)\n",
    "\n",
    "#Summary 2: Create Loss Function & Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "#Summary 3: Setup SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "_MODEL_NAME_ = \"model50_test_trash\" #NOTE: CHANGE ME\n",
    "writer = SummaryWriter(\"runs/\" + _MODEL_NAME_)\n",
    "\n",
    "#Summary 4: Setup Epochs\n",
    "epochs = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:37.589970Z",
     "start_time": "2024-07-26T16:51:37.451712Z"
    }
   },
   "id": "8f12cb9897940088"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "#Add Model Structure To TensorBoard\n",
    "coeff,y = next(iter(test_dataloader))\n",
    "writer.add_graph(model, coeff.to(device))\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:37.631781Z",
     "start_time": "2024-07-26T16:51:37.460072Z"
    }
   },
   "id": "a7e9d44c10649ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55374a0a171246ba86db34d6b4e1e5b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/280 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e07f3b5b7ad46b0be99440bfbb1b830"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 4.006 | Test loss: 3.905 | Train Acc: 3.107142448425293 | Test Acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    #ii TRAINING loop\n",
    "    for batch, (X, y) in enumerate(tqdm(train_dataloader)):\n",
    "        #Note 0. Turn On Training Mode\n",
    "        model.train()\n",
    "        #Note 0.1. Put data on CPU\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        #Note 0.2. Check That Data has values - #ERROR This can be deleted quite honestly\n",
    "        if torch.isnan(X).any() or torch.isinf(X).any():\n",
    "            print(\"Data contains NaN or Inf values.\")\n",
    "\n",
    "        #Note 1. Forward pass\n",
    "        logits = model(X)\n",
    "        #Note 1.1 Convert Raw Logit Outputs to y predictions\n",
    "        y_pred = torch.softmax(logits, dim = 1)  #;print(y_pred.shape)\n",
    "        #Note 1.2 Convert y predictions to labels as y hat\n",
    "        y_hat = torch.argmax(y_pred, dim = 1)  #;print(y_hat.shape);print(y_hat)\n",
    "\n",
    "        #Note 2. Loss calculation\n",
    "        loss = loss_fn(logits, y)\n",
    "        #Note 2.1 Cumulatively add up the LOSS per epoch\n",
    "        train_loss += loss\n",
    "        #Note 2.1 Cumulatively add up the ACCURACY per epoch\n",
    "        train_acc += acc_func(y, y_hat)  #this slows down the runtime?\n",
    "\n",
    "        #Note 3. Optimizer zero grad - reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Note 4. Loss backward - Backward propagation\n",
    "        loss.backward()\n",
    "        #Note 5. Step the optimizer - optimize\\upgade weight\\biases\\paramters of kernels\n",
    "        optimizer.step()\n",
    "\n",
    "    #Summary: Adjust TRAIN LOSS & ACCURACY for number of batches \n",
    "    #SUMMARY: Meaning -> Divide total TRAIN LOSS & ACC by length of TRAIN DATALOADER = (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "\n",
    "    #ii TESTING loop\n",
    "    #Note 0. init Metric Params\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    #Note 1. Put model in eval mode \n",
    "    model.eval()\n",
    "\n",
    "    #Note 1.1 Turn on inference mode - best way currently to do evaluation\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X_test, y_test) in enumerate(test_dataloader):\n",
    "            #Note 2. Make sure test data on CPU\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            #Note 3. Forward Pass\n",
    "            logits = model(X_test)\n",
    "            #Note 3.1 Convert Raw Logits (outputs) to y predictions\n",
    "            y_pred = torch.softmax(logits, dim = 1)  #;print(y_pred.shape)\n",
    "            #Note 3.2 Convert y predictions to y labels as y hat\n",
    "            y_hat = torch.argmax(y_pred, dim = 1)\n",
    "\n",
    "            \n",
    "            \n",
    "            #Note 4. Calculate The Loss\n",
    "            loss = loss_fn(logits, y_test)\n",
    "            #Note 4.1 Cumulatively add up the LOSS per epoch (all batches)\n",
    "            test_loss += loss\n",
    "            #Note 4.2 Cumulatively add up the ACCURACY per epoch (all batches)\n",
    "            test_acc += acc_func(y, y_hat)  #this slows down the runtime\n",
    "        \n",
    "        #Summary: Adjust TEST LOSS & ACCURACY for number of batches \n",
    "        #SUMMARY: Meaning -> Divide total TEST LOSS & ACC by length of TEST DATALOADER = (average loss per batch per epoch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    # Print out what's happening\n",
    "    print(f\"Epoch: {epoch} | Loss: {train_loss:.3f} | Test loss: {test_loss:.3f} | Train Acc: {train_acc * 100} | Test Acc: {test_acc * 100}\")\n",
    "    \n",
    "    #Note: Tensorboard Logging\n",
    "    #writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    #writer.add_scalar(\"Loss/Test\", test_loss, epoch)\n",
    "    #writer.add_scalar(\"Accuracy/Train\", train_acc*100, epoch)\n",
    "    #writer.add_scalar(tag = \"Accuracy/Test\", scalar_value = test_acc*100,global_step =  epoch)\n",
    "    \n",
    "    writer.add_scalars(\"Loss\",{'Train':train_loss},epoch)\n",
    "    writer.add_scalars(\"Loss\",{'Test':test_loss},epoch)\n",
    "\n",
    "    \n",
    "    writer.add_scalars(\"Accuracy\",{'Train':train_acc*100},epoch)\n",
    "    writer.add_scalars(\"Accuracy\",{'Test':test_acc*100},epoch)\n",
    "\n",
    "    writer.add_scalars(\"Train Loss/Accuracy\",{'Loss':train_loss},epoch)\n",
    "    writer.add_scalars(\"Train Loss/Accuracy\",{'Accuracy':train_acc*100},epoch)\n",
    "\n",
    "    writer.add_scalars(\"Test Loss/Accuracy\",{'Loss':test_loss},epoch)\n",
    "    writer.add_scalars(\"Test Loss/Accuracy\",{'Accuracy':test_acc*100},epoch)\n",
    "\n",
    "\n",
    "    #Call flush() method to make sure that all pending events have been written to disk.\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:43.959137Z",
     "start_time": "2024-07-26T16:51:38.218629Z"
    }
   },
   "id": "6e03ff4858895a60"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:47:53.957102Z",
     "start_time": "2024-07-26T16:47:53.850452Z"
    }
   },
   "id": "24b47ab974fa72e4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4636dbe132a09f17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
